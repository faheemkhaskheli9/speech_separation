{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Looking to listen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TORRBkKk38lB"
      },
      "source": [
        "## Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsC8Eedl0l6R",
        "outputId": "1f6067aa-f6db-4f2b-fd58-d4f89fa9bd11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEZSmmaE0t36",
        "outputId": "64c6e917-952d-4ece-dc58-70780fa3dfa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd 'My Drive/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8mY4YWWzzbQ"
      },
      "source": [
        "# go to project folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taxRn0Dd0v_5",
        "outputId": "6f5aa3e5-9c6b-46e6-8f9e-8f4ee4113da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd speech_separation"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/speech_separation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQWeju-n4Fho"
      },
      "source": [
        "### intsall packages needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC3m4IoPghhr",
        "outputId": "99924d2e-3804-4b86-f8cd-9de73b328770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW8Iokd7mW1J",
        "outputId": "6a16c79b-8e37-4b74-e6ed-d03857eb5a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install youtube-dl"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube-dl in /usr/local/lib/python3.6/dist-packages (2020.9.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIBwJ-uvrDgY",
        "outputId": "27061524-5f37-4a17-cc0f-94cb7801dcae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install libsox-fmt-all libsox-dev sox"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsox-dev is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "libsox-fmt-all is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "sox is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-T-XaFwgNbB"
      },
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-pWBSVj4jCw"
      },
      "source": [
        "from data.lib import AVHandler as avh"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwMIVNMzgPda"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "import datetime"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laXlCLlJgVeL"
      },
      "source": [
        "from mtcnn.mtcnn import MTCNN\n",
        "import cv2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFfoRn5XgZ1X"
      },
      "source": [
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "\n",
        "from model.lib import utils\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "import math"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRbI0XJAgrcP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import tensor_util\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDkFusr-g2vp"
      },
      "source": [
        "from keras import optimizers\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Convolution2D, Bidirectional, UpSampling2D, UpSampling3D, concatenate\n",
        "from keras.layers import Dropout, Flatten, BatchNormalization, ReLU, Reshape, Permute, Lambda, TimeDistributed\n",
        "from keras.models import Model, load_model \n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.initializers import he_normal, glorot_uniform\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n",
        "from keras.callbacks import TensorBoard\n",
        "from model.lib.MyGenerator import AVGenerator"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By97aLgthAsH"
      },
      "source": [
        "from model.lib.model_ops import ModelMGPU,latest_file\n",
        "from keras import optimizers\n",
        "from model.lib.model_loss import audio_discriminate_loss2 as audio_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmfaZ5zV4YxQ"
      },
      "source": [
        "### Dataset Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_UW_8OcHvZd"
      },
      "source": [
        "cat_train = pd.read_csv('data/audio/catalog/avspeech_train.csv')\n",
        "cat_train.columns = ['link','start_time','end_time','pos_x','pos_y']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzt2x5hs2ZMU",
        "outputId": "b9b719a9-8ee5-47ac-c7cd-d66ba6f656d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cat_train.values.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2621844, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrhMlT6CmEqT"
      },
      "source": [
        "## Dataset Samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ5goeHLhtMS"
      },
      "source": [
        "# range for our dataset to be used\n",
        "data_range = (1, 1000 ) # start index and ending index"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wohG74tAkFd"
      },
      "source": [
        "# Step 2 - download audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duLeGAVb2v2E",
        "outputId": "56fdfce7-6052-45fc-f0de-6248ff111d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Before running, make sure avspeech_train.csv and avspeech_test.csv are in catalog.\n",
        "# if not, see the requirement.txt\n",
        "# download and preprocess the data from AVspeech dataset\n",
        "\n",
        "def m_link(youtube_id):\n",
        "    # return the youtube actual link\n",
        "    link = 'https://www.youtube.com/watch?v='+youtube_id\n",
        "    return link\n",
        "\n",
        "def m_audio(loc,name,cat,start_idx,end_idx):\n",
        "    # make concatenated audio following by the catalog from AVSpeech\n",
        "    # loc       | the location for file to store\n",
        "    # name      | name for the wav mix file\n",
        "    # cat       | the catalog with audio link and time\n",
        "    # start_idx | the starting index of the audio to download and concatenate\n",
        "    # end_idx   | the ending index of the audio to download and concatenate\n",
        "\n",
        "    for i in range(start_idx,end_idx):\n",
        "        \n",
        "        f_name = name+str(i)\n",
        "        link = m_link(cat.loc[i,'link'])\n",
        "        start_time = cat.loc[i,'start_time']\n",
        "        end_time = start_time + 3.0\n",
        "        avh.download(loc,f_name,link)\n",
        "        avh.cut(loc,f_name,start_time,end_time)\n",
        "        print('\\r',i,'/',end_idx,end='')\n",
        "\n",
        "# create 80000-90000 audios data from 290K\n",
        "avh.mkdir('data/audio/audio_train')\n",
        "\n",
        "start_time = time.time()\n",
        "m_audio('data/audio/audio_train','audio_train',cat_train,data_range[0],data_range[1])\n",
        "print( 'Time Taken', time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 404 / 1000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTODsPfzg5I"
      },
      "source": [
        "# Step 3 - Normalize audio data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOM0ANOaqMF8"
      },
      "source": [
        "RANGE = data_range\n",
        "\n",
        "if(not os.path.isdir('data/audio/norm_audio_train')):\n",
        "    os.mkdir('data/audio/norm_audio_train')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for num in range(RANGE[0],RANGE[1]+1):\n",
        "    path = './data/audio/audio_train/trim_audio_train%s.wav'% num\n",
        "    norm_path = './data/audio/norm_audio_train/trim_audio_train%s.wav'% num\n",
        "    if (os.path.exists(path)):\n",
        "        audio,_= librosa.load(path,sr=16000)\n",
        "        max = np.max(np.abs(audio))\n",
        "        norm_audio = np.divide(audio,max)\n",
        "        wavfile.write(norm_path,16000,norm_audio)\n",
        "\n",
        "print( 'Time Taken', time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHcFFvDyAg8t"
      },
      "source": [
        "# Step 4 - Downlaod videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KWsvrdm4CLH"
      },
      "source": [
        "def video_download(loc,cat,start_idx,end_idx):\n",
        "    # Only download the video from the link\n",
        "    # loc        | the location for downloaded file\n",
        "    # v_name     | the name for the video file\n",
        "    # cat        | the catalog with audio link and time\n",
        "    # start_idx  | the starting index of the video to download\n",
        "    # end_idx    | the ending index of the video to download\n",
        "\n",
        "    for i in range(start_idx,end_idx):\n",
        "        command = 'cd %s;' % loc\n",
        "        f_name = str(i)\n",
        "        link = avh.m_link(cat.loc[i, 'link'])\n",
        "        start_time = cat.loc[i, 'start_time']\n",
        "        end_time = start_time + 3.0\n",
        "        start_time = datetime.timedelta(seconds=start_time)\n",
        "        end_time = datetime.timedelta(seconds=end_time)\n",
        "        command += 'ffmpeg -i $(youtube-dl -f ”mp4“ --get-url ' + link + ') ' + '-c:v h264 -c:a copy -ss %s -to %s %s.mp4' \\\n",
        "                % (start_time, end_time, f_name)\n",
        "        #command += 'ffmpeg -i %s.mp4 -r 25 %s.mp4;' % (f_name,'clip_' + f_name) #convert fps to 25\n",
        "        #command += 'rm %s.mp4' % f_name\n",
        "        os.system(command)\n",
        "        print('\\r',i,'/',end_idx,end='')\n",
        "\n",
        "def generate_frames(loc,start_idx,end_idx):\n",
        "    # get frames for each video clip\n",
        "    # loc        | the location of video clip\n",
        "    # v_name     | v_name = 'clip_video_train'\n",
        "    # start_idx  | the starting index of the training sample\n",
        "    # end_idx    | the ending index of the training sample\n",
        "\n",
        "    avh.mkdir('data/video/frames')\n",
        "    for i in range(start_idx, end_idx):\n",
        "        command = 'cd %s;' % loc\n",
        "        f_name = str(i)\n",
        "        command += 'ffmpeg -i %s.mp4 -y -f image2  -vframes 75 ../video/frames/%s-%%02d.jpg' % (f_name, f_name)\n",
        "        os.system(command)\n",
        "\n",
        "\n",
        "def download_video_frames(loc,cat,start_idx,end_idx,rm_video):\n",
        "    # Download each video and convert to frames immediately, can choose to remove video file\n",
        "    # loc        | the location for downloaded file\n",
        "    # cat        | the catalog with audio link and time\n",
        "    # start_idx  | the starting index of the video to download\n",
        "    # end_idx    | the ending index of the video to download\n",
        "    # rm_video   | boolean value for delete video and only keep the frames\n",
        "\n",
        "    avh.mkdir('data/video/frames')\n",
        "    for i in range(start_idx, end_idx + 1):\n",
        "        command = 'cd %s;' % loc\n",
        "        f_name = str(i)\n",
        "        link = avh.m_link(cat.loc[i, 'link'])\n",
        "        start_time = cat.loc[i, 'start_time']\n",
        "        end_time = start_time + 3.0\n",
        "        start_time = datetime.timedelta(seconds=start_time)\n",
        "        end_time = datetime.timedelta(seconds=end_time)\n",
        "        command += 'ffmpeg -i $(youtube-dl -f ”mp4“ --get-url ' + link + ') ' + '-c:v h264 -c:a copy -ss %s -to %s %s.mp4;' \\\n",
        "                   % (start_time, end_time, f_name)\n",
        "        #ommand += 'ffmpeg -i %s.mp4 -r 25 %s.mp4;' % (f_name, 'clip_' + f_name)  # convert fps to 25\n",
        "        #command += 'rm %s.mp4;' % f_name\n",
        "\n",
        "        #converts to frames\n",
        "        #command += 'ffmpeg -i %s.mp4 -y -f image2  -vframes 75 ../frames/%s-%%02d.jpg;' % (f_name, f_name)\n",
        "        command += 'ffmpeg -i %s.mp4 -vf fps=25 ../video/frames/%s-%%02d.jpg;' % (f_name, f_name)\n",
        "        #command += 'ffmpeg -i %s.mp4 ../frames/%sfr_%%02d.jpg;' % ('clip_' + f_name, f_name)\n",
        "\n",
        "        if rm_video:\n",
        "            command += 'rm %s.mp4' % f_name\n",
        "        os.system(command)\n",
        "\n",
        "avh.mkdir('data/video/video_train')\n",
        "\n",
        "# download video , convert to images separately\n",
        "#avh.video_download(loc='video_train',v_name='video_train',cat=cat_train,start_idx=2,end_idx=4)\n",
        "#avh.generate_frames(loc='video_train',v_name='clip_video_train',start_idx=2,end_idx=4)\n",
        "# download each video and convert to frames immediately\n",
        "\n",
        "start_time = time.time()\n",
        "download_video_frames(loc='data/video/video_train',cat=cat_train,start_idx=data_range[0],end_idx=data_range[1],rm_video=False)\n",
        "print( 'Time Taken', time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXHfdnuTAvrB"
      },
      "source": [
        "# Step 5 - detect face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdpv2AvfAnE1"
      },
      "source": [
        "frame_path = './data/video/frames/'\n",
        "output_dir = './data/video/face_input'\n",
        "\n",
        "if not os.path.isdir('./data/video/face_input'):\n",
        "    os.mkdir('./data/video/face_input')\n",
        "\n",
        "def bounding_box_check(faces,x,y):\n",
        "    # check the center\n",
        "    for face in faces:\n",
        "        bounding_box = face['box']\n",
        "        if(bounding_box[1]<0):\n",
        "            bounding_box[1] = 0\n",
        "        if(bounding_box[0]<0):\n",
        "            bounding_box[0] = 0\n",
        "        if(bounding_box[0]-50>x or bounding_box[0]+bounding_box[2]+50<x):\n",
        "            print('change person from')\n",
        "            print(bounding_box)\n",
        "            print('to')\n",
        "            continue\n",
        "        if (bounding_box[1]-50 > y or bounding_box[1] + bounding_box[3]+50 < y):\n",
        "            print('change person from')\n",
        "            print(bounding_box)\n",
        "            print('to')\n",
        "            continue\n",
        "        return bounding_box\n",
        "\n",
        "def face_detect(file,detector,frame_path=frame_path,cat_train=cat_train):\n",
        "    name = file.replace('.jpg', '').split('-')\n",
        "    log = cat_train.iloc[int(name[0])]\n",
        "    x = log['pos_x']\n",
        "    y = log['pos_y']\n",
        "\n",
        "    img = cv2.imread('%s%s'%(frame_path,file))\n",
        "    x = img.shape[1] * x\n",
        "    y = img.shape[0] * y\n",
        "    faces = detector.detect_faces(img)\n",
        "    # check if detected faces\n",
        "    if(len(faces)==0):\n",
        "        print('no face detect: '+file)\n",
        "        return #no face\n",
        "    bounding_box = bounding_box_check(faces,x,y)\n",
        "    if(bounding_box == None):\n",
        "        print('face is not related to given coord: '+file)\n",
        "        return\n",
        "    print(file,\" \",bounding_box)\n",
        "    print(file,\" \",x, y)\n",
        "    crop_img = img[bounding_box[1]:bounding_box[1] + bounding_box[3],bounding_box[0]:bounding_box[0]+bounding_box[2]]\n",
        "    crop_img = cv2.resize(crop_img,(160,160))\n",
        "    cv2.imwrite('%s/frame_'%output_dir + name[0] + '_' + name[1] + '.jpg', crop_img)\n",
        "    #crop_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
        "    #plt.imshow(crop_img)\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "detector = MTCNN()\n",
        "for i in range(data_range[0],data_range[1]+1):\n",
        "    for j in range(1,76):\n",
        "        file_name = \"%d-%02d.jpg\"%(i, j)\n",
        "        if (not os.path.exists('%s%s' % (frame_path, file_name))):\n",
        "            print('cannot find input: ' + '%s%s' % (frame_path, file_name))\n",
        "            continue\n",
        "        face_detect(file_name, detector)\n",
        "\n",
        "print( 'Time Taken', time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvD2PWyQzRey"
      },
      "source": [
        "### valid or not valid frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgm49ZVbBJP3"
      },
      "source": [
        "inspect_dir = 'data/video/face_input'\n",
        "inspect_range = data_range\n",
        "valid_frame_path = 'data/video/valid_frame.txt'\n",
        "\n",
        "with open(valid_frame_path,'w+') as f:\n",
        "  pass\n",
        "\n",
        "def check_frame(idx,part,dir=inspect_dir):\n",
        "    path = dir + \"/frame_%d_%02d.jpg\"%(idx,part)\n",
        "#    print(path)\n",
        "    if(not os.path.exists(path)): return False\n",
        "    return True\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(inspect_range[0],inspect_range[1]+1):\n",
        "    valid = True\n",
        "#    print('processing frame %s'%i)\n",
        "    for j in range(1,76):\n",
        "        if(check_frame(i,j)==False):\n",
        "            path = inspect_dir + \"/frame_%d_*.jpg\"% i\n",
        "            for file in glob.glob(path):\n",
        "                os.remove(file)\n",
        "            valid = False\n",
        "            print('frame %s is not valid'%i,j)\n",
        "            break\n",
        "    if valid:\n",
        "        with open(valid_frame_path,'a') as f:\n",
        "            frame_name = \"frame_%d\"%i\n",
        "            f.write(frame_name+'\\n')\n",
        "            print(frame_name)\n",
        "\n",
        "print( 'Time Taken', time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izHH5Q1hzXI2"
      },
      "source": [
        "# Step 6 - Create audio database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5E7Y9AoBn-F"
      },
      "source": [
        "# Parameter\n",
        "SAMPLE_RANGE = data_range # data usage to generate database\n",
        "WAV_REPO_PATH = os.path.expanduser(\"./data/audio/norm_audio_train\")\n",
        "DATABASE_REPO_PATH = './data/audio/AV_model_database'\n",
        "FRAME_LOG_PATH = './data/video/valid_frame.txt'\n",
        "NUM_SPEAKER = 3\n",
        "MAX_NUM_SAMPLE = 50000\n",
        "\n",
        "# time measure decorator\n",
        "def timit(func):\n",
        "    def cal_time(*args,**kwargs):\n",
        "        tic = time.time()\n",
        "        result = func(*args,**kwargs)\n",
        "        tac = time.time()\n",
        "        print(func.__name__,'running time: ',(tac-tic),'ms')\n",
        "        return result\n",
        "    return cal_time\n",
        "\n",
        "# create directory to store database\n",
        "def init_dir(path = DATABASE_REPO_PATH ):\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "    if not os.path.isdir('%s/mix'%path):\n",
        "        os.mkdir('%s/mix'%path)\n",
        "\n",
        "    if not os.path.isdir('%s/single'%path):\n",
        "        os.mkdir('%s/single'%path)\n",
        "\n",
        "    if not os.path.isdir('%s/crm'%path):\n",
        "        os.mkdir('%s/crm'%path)\n",
        "\n",
        "    if not os.path.isdir('%s/mix_wav'%path):\n",
        "        os.mkdir('%s/mix_wav'%path)\n",
        "\n",
        "@timit\n",
        "def generate_path_list(sample_range=SAMPLE_RANGE,repo_path=WAV_REPO_PATH,frame_path=FRAME_LOG_PATH):\n",
        "    '''\n",
        "\n",
        "    :param sample_range:\n",
        "    :param repo_path:\n",
        "    :return: 2D array with idx and path (idx_wav,path_wav)\n",
        "    '''\n",
        "    audio_path_list = []\n",
        "    frame_set = set()\n",
        "    \n",
        "    with open(frame_path,'r') as f:\n",
        "        frames = f.readlines()\n",
        "    \n",
        "    for i in range(len(frames)):\n",
        "        frame = frames[i].replace('\\n','').replace('frame_','')\n",
        "        frame_set.add(int(frame))\n",
        "    \n",
        "    for i in range(sample_range[0],(sample_range[1]+1)):\n",
        "        print('\\rchecking...%d'%int(frame),end='')\n",
        "        path = repo_path + '/trim_audio_train%d.wav'%i\n",
        "        if os.path.exists(path) and (i in frame_set):\n",
        "            audio_path_list.append((i,path))\n",
        "    print('\\nlength of the path list: ',len(audio_path_list))\n",
        "    return audio_path_list\n",
        "\n",
        "# data generate function\n",
        "def single_audio_to_npy(audio_path_list,database_repo=DATABASE_REPO_PATH,fix_sr=16000):\n",
        "    for idx,path in audio_path_list:\n",
        "        print('\\rsingle npy generating... %d'%((idx/len(audio_path_list))*100),end='')\n",
        "        data, _ = librosa.load(path, sr=fix_sr)\n",
        "        data = utils.fast_stft(data)\n",
        "        name = 'single-%05d'%idx\n",
        "        with open('%s/single_TF.txt'%database_repo,'a') as f:\n",
        "            f.write('%s.npy'%name)\n",
        "            f.write('\\n')\n",
        "        np.save(('%s/single/%s.npy'%(database_repo,name)),data)\n",
        "    print()\n",
        "\n",
        "\n",
        "# split single TF data to different part in order to mix\n",
        "def split_to_mix(audio_path_list,database_repo=DATABASE_REPO_PATH,partition=2):\n",
        "    # return split_list : (part1,part2,...)\n",
        "    # each part : (idx,path)\n",
        "    length = len(audio_path_list)\n",
        "    part_len = length // partition\n",
        "    head = 0\n",
        "    part_idx = 0\n",
        "    split_list = []\n",
        "    print('length = ',length,', parts = ',part_len)\n",
        "    if part_len == 0:\n",
        "      part_len = 1\n",
        "    while((head+part_len)<=length):\n",
        "        part = audio_path_list[head:(head+part_len)]\n",
        "        split_list.append(part)\n",
        "        with open('%s/single_TF_part%d.txt'%(database_repo,part_idx),'a') as f:\n",
        "            for idx, _ in part:\n",
        "                name = 'single-%05d' % idx\n",
        "                f.write('%s.npy' % name)\n",
        "                f.write('\\n')\n",
        "                print('\\r',idx, end='')\n",
        "        head += part_len\n",
        "        part_idx += 1\n",
        "    print('done')\n",
        "    return split_list\n",
        "\n",
        "# mix single TF data\n",
        "def all_mix(split_list,database_repo=DATABASE_REPO_PATH,partition=2):\n",
        "    print(len(split_list))\n",
        "    assert len(split_list) == partition\n",
        "    print('mixing data...')\n",
        "    num_mix = 1\n",
        "    num_mix_check = 0\n",
        "    for part in split_list:\n",
        "        num_mix *= len(part)\n",
        "    print ('number of mix data; ',num_mix)\n",
        "\n",
        "    part_len = len(split_list[-1])\n",
        "    idx_list = [x for x in range(part_len)]\n",
        "    combo_idx_list = itertools.product(idx_list,repeat=partition)\n",
        "    for combo_idx in combo_idx_list:\n",
        "        num_mix_check +=1\n",
        "        single_mix(combo_idx,split_list,database_repo)\n",
        "        print('\\rnum of completed mixing audio : %d'%num_mix_check,end='')  \n",
        "    print()\n",
        "# mix several wav file and store TF domain data with npy\n",
        "def single_mix(combo_idx,split_list,database_repo):\n",
        "    assert len(combo_idx) == len(split_list)\n",
        "    mix_rate = 1.0 / float(len(split_list))\n",
        "    wav_list = []\n",
        "    prefix = \"mix\"\n",
        "    mid_name = \"\"\n",
        "\n",
        "    for part_idx in range(len(split_list)):\n",
        "        idx,path = split_list[part_idx][combo_idx[part_idx]]\n",
        "        wav, _ = librosa.load(path, sr=16000)\n",
        "        wav_list.append(wav)\n",
        "        mid_name += '-%05d' % idx\n",
        "\n",
        "    # mix wav file\n",
        "    mix_wav = np.zeros_like(wav_list[0])\n",
        "    for wav in wav_list:\n",
        "        mix_wav += wav * mix_rate\n",
        "\n",
        "    # save mix wav file\n",
        "    wav_name = prefix+mid_name+'.wav'\n",
        "    wavfile.write('%s/mix_wav/%s'%(database_repo,wav_name),16000,mix_wav)\n",
        "\n",
        "    # transfer mix wav to TF domain\n",
        "    F_mix = utils.fast_stft(mix_wav)\n",
        "    name = prefix+mid_name+\".npy\"\n",
        "    store_path = '%s/mix/%s'%(database_repo,name)\n",
        "\n",
        "    # save mix as npy file\n",
        "    np.save(store_path,F_mix)\n",
        "\n",
        "    # save mix log\n",
        "    with open('%s/mix_log.txt'%database_repo,'a') as f:\n",
        "        f.write(name)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "\n",
        "def all_crm(mix_log_path,database_repo=DATABASE_REPO_PATH):\n",
        "    with open(mix_log_path,'r') as f:\n",
        "        mix_list = f.read().splitlines()\n",
        "\n",
        "    for mix in mix_list:\n",
        "        mix_path = '%s/mix/%s' % (database_repo, mix)\n",
        "        mix = mix.replace(\".npy\",\"\")\n",
        "        mix = mix.replace(\"mix-\",\"\")\n",
        "        idx_str_list = mix.split(\"-\")\n",
        "        single_crm(idx_str_list,mix_path,database_repo)\n",
        "\n",
        "def single_crm(idx_str_list,mix_path,database_repo):\n",
        "    F_mix = np.load(mix_path)\n",
        "    mid_name = \"\"\n",
        "    mix_name = \"mix\"\n",
        "    dataset_line = \"\"\n",
        "\n",
        "    for idx in idx_str_list:\n",
        "        mid_name += \"-%s\"%idx\n",
        "        mix_name += \"-%s\"%idx\n",
        "    mix_name += '.npy'\n",
        "    dataset_line += mix_name\n",
        "\n",
        "    for idx in idx_str_list:\n",
        "        single_name = 'single-%s.npy'%idx\n",
        "        path = '%s/single/%s'%(database_repo,single_name)\n",
        "        F_single = np.load(path)\n",
        "        cRM = utils.fast_cRM(F_single,F_mix)\n",
        "\n",
        "        last_name = '-%s'%idx\n",
        "        cRM_name = 'crm' + mid_name + last_name + '.npy'\n",
        "\n",
        "        # save crm to npy\n",
        "        store_path = '%s/crm/%s'%(database_repo,cRM_name)\n",
        "        np.save(store_path,cRM)\n",
        "\n",
        "        # save crm information to log\n",
        "        with open('%s/crm_log.txt'%database_repo, 'a') as f:\n",
        "            f.write(cRM_name)\n",
        "            f.write('\\n')\n",
        "        dataset_line += (\" \"+cRM_name)\n",
        "\n",
        "    # write in database log\n",
        "    with open('%s/dataset.txt'%database_repo,'a') as f:\n",
        "        f.write(dataset_line)\n",
        "        f.write('\\n')\n",
        "\n",
        "\n",
        "def train_test_split(dataset_log_path,data_range=[0,50000],test_ratio=0.1,shuffle=True,database_repo=DATABASE_REPO_PATH):\n",
        "    with open(dataset_log_path,'r') as f:\n",
        "        data_log = f.read().splitlines()\n",
        "\n",
        "    if data_range[1]> len(data_log):\n",
        "        data_range[1] = len(data_log)-1\n",
        "    samples = data_log[data_range[0]:data_range[1]]\n",
        "    if shuffle:\n",
        "        random.shuffle(samples)\n",
        "\n",
        "    length = len(samples)\n",
        "    mid = int(math.floor(test_ratio*length))\n",
        "    test = samples[:mid]\n",
        "    train = samples[mid:]\n",
        "\n",
        "    with open('%s/dataset_train.txt'%database_repo,'a') as f:\n",
        "        for line in train:\n",
        "            f.write(line)\n",
        "            f.write('\\n')\n",
        "\n",
        "    with open('%s/dataset_val.txt' % database_repo, 'a') as f:\n",
        "        for line in test:\n",
        "            f.write(line)\n",
        "            f.write('\\n')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init_dir()\n",
        "    audio_path_list = generate_path_list()\n",
        "    print('\\nsingle audio to npy')\n",
        "    single_audio_to_npy(audio_path_list)\n",
        "\n",
        "    print('\\nsplit to mix')\n",
        "    start_time = time.time()\n",
        "    split_list = split_to_mix(audio_path_list,partition=NUM_SPEAKER)\n",
        "    print( time.time() - start_time )\n",
        "\n",
        "    print('\\nall mix')\n",
        "    start_time = time.time()\n",
        "    all_mix(split_list,partition=NUM_SPEAKER)\n",
        "    print( time.time() - start_time )\n",
        "\n",
        "    print('\\nall crm')\n",
        "    start_time = time.time()\n",
        "    mix_log_path = '%s/mix_log.txt'%DATABASE_REPO_PATH\n",
        "    all_crm(mix_log_path)\n",
        "    print( time.time() - start_time )\n",
        "\n",
        "    print('\\ntrain test split')\n",
        "    start_time = time.time()\n",
        "    dataset_log_path ='%s/dataset.txt'%DATABASE_REPO_PATH\n",
        "    train_test_split(dataset_log_path,data_range=[0,MAX_NUM_SAMPLE])\n",
        "    print( time.time() - start_time )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF9yGcV1xQdA"
      },
      "source": [
        "# Step 7 - Generate log file for data generator\n",
        "\n",
        "create AVdataset_train.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KQHTMlnCwpa"
      },
      "source": [
        "with open('./data/audio/AV_model_database/dataset_train.txt', 'r') as t:\n",
        "    lines = t.readlines()\n",
        "    for line in lines:\n",
        "        info = line.strip().split('.')\n",
        "        num1 = info[0].strip().split('-')[1]\n",
        "        num2 = info[0].strip().split('-')[2]\n",
        "\n",
        "        newline = line.strip() + ' ' + num1 + '_face_emb.npy' + ' ' + num2 + '_face_emb.npy\\n'\n",
        "        with open('./data/AV_log/AVdataset_train.txt', 'a') as f:\n",
        "            f.write(newline)\n",
        "\n",
        "with open('./data/audio/AV_model_database/dataset_val.txt', 'r') as t:\n",
        "    lines = t.readlines()\n",
        "    for line in lines:\n",
        "        info = line.strip().split('.')\n",
        "        num1 = info[0].strip().split('-')[1]\n",
        "        num2 = info[0].strip().split('-')[2]\n",
        "\n",
        "        newline = line.strip() + ' ' + num1 + '_face_emb.npy' + ' ' + num2 + '_face_emb.npy\\n'\n",
        "        with open('./data/AV_log/AVdataset_val.txt', 'a') as f:\n",
        "            f.write(newline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qCXz8OHOYev"
      },
      "source": [
        "# Step 8 - Generate Face Embedding\n",
        "\n",
        "download facenet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1RpSbD5OV4x"
      },
      "source": [
        "start_time = time.time()\n",
        "## paraeter\n",
        "PB = 0\n",
        "CKPT = 0\n",
        "HDF5 = 1\n",
        "MODEL_PATH = 'FaceNet_keras/facenet_keras.h5'\n",
        "VALID_FRAME_LOG_PATH = './data/video/valid_frame.txt'\n",
        "FACE_INPUT_PATH = './data/video/face_input/'\n",
        "\n",
        "data = np.random.randint(256,size=(1,160,160,3),dtype='int32')\n",
        "\n",
        "###############\n",
        "#graph_path = 'FaceNet_new/20180402-114759.pb'\n",
        "# utils.inspect_operation(graph_path,'ops.txt')\n",
        "if PB:\n",
        "    with tf.gfile.FastGFile(graph_path,'rb') as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "        with tf.Session() as sess:\n",
        "            #sess.graph.as_default()\n",
        "            tf.import_graph_def(graph_def)\n",
        "            print(sess.run('import/embeddings', feed_dict={'import/batch_size:0': data,'import/phase_train:0':False}))\n",
        "\n",
        "if CKPT:\n",
        "    saver = tf.train.import_meta_graph('FaceNet_new/model-20180402-114759.meta')\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess=sess,save_path='FaceNet_new/model-20180402-114759.ckpt-275')\n",
        "        #print(sess.run('embeddings:0', feed_dict={'batch_size:0': data, 'phase_train:0': False}))\n",
        "\n",
        "if HDF5:\n",
        "    save_path = './data/video/face_emb/'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "\n",
        "    model = load_model(MODEL_PATH)\n",
        "    model.summary()\n",
        "    avgPool_layer_model = Model(inputs=model.input,outputs=model.get_layer('AvgPool').output)\n",
        "    # print(avgPool_layer_model.predict(data))\n",
        "\n",
        "    lines = []\n",
        "    with open(VALID_FRAME_LOG_PATH, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        embtmp = np.zeros((75, 1, 1792))\n",
        "        headname = line.strip()\n",
        "        tailname = ''\n",
        "        for i in range(1, 76):\n",
        "            if i < 10:\n",
        "                tailname = '_0{}.jpg'.format(i)\n",
        "            else:\n",
        "                tailname = '_' + str(i) + '.jpg'\n",
        "            picname = headname + tailname\n",
        "            # print(picname)\n",
        "            I = mpimg.imread(FACE_INPUT_PATH + picname)\n",
        "            I_np = np.array(I)\n",
        "            I_np = I_np[np.newaxis, :, :, :]\n",
        "            # print(I_np.shape)\n",
        "            # print(avgPool_layer_model.predict(I_np).shape)\n",
        "            embtmp[i - 1, :] = avgPool_layer_model.predict(I_np)\n",
        "\n",
        "        # print(embtmp.shape)\n",
        "        people_index = int(line.strip().split('_')[1])\n",
        "        npname = '{:05d}_face_emb.npy'.format(people_index)\n",
        "        print(npname)\n",
        "\n",
        "        np.save(save_path + npname, embtmp)\n",
        "        with open('faceemb_dataset.txt', 'a') as d:\n",
        "            d.write(npname + '\\n')\n",
        "\n",
        "print( 'Time Taken', time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS7X_m51UnW4"
      },
      "source": [
        "def AV_model(people_num=2):\n",
        "    def UpSampling2DBilinear(size):\n",
        "        return Lambda(lambda x: tf.compat.v1.image.resize_bilinear(x, size, align_corners=True))\n",
        "\n",
        "    def sliced(x, index):\n",
        "        return x[:, :, :, index]\n",
        "\n",
        "    # --------------------------- AS start ---------------------------\n",
        "    audio_input = Input(shape=(298, 257, 2))\n",
        "    print('as_0:', audio_input.shape)\n",
        "    as_conv1 = Convolution2D(96, kernel_size=(1, 7), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv1')(audio_input)\n",
        "    as_conv1 = BatchNormalization()(as_conv1)\n",
        "    as_conv1 = ReLU()(as_conv1)\n",
        "    print('as_1:', as_conv1.shape)\n",
        "\n",
        "    as_conv2 = Convolution2D(96, kernel_size=(7, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv2')(as_conv1)\n",
        "    as_conv2 = BatchNormalization()(as_conv2)\n",
        "    as_conv2 = ReLU()(as_conv2)\n",
        "    print('as_2:', as_conv2.shape)\n",
        "\n",
        "    as_conv3 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv3')(as_conv2)\n",
        "    as_conv3 = BatchNormalization()(as_conv3)\n",
        "    as_conv3 = ReLU()(as_conv3)\n",
        "    print('as_3:', as_conv3.shape)\n",
        "\n",
        "    as_conv4 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(2, 1), name='as_conv4')(as_conv3)\n",
        "    as_conv4 = BatchNormalization()(as_conv4)\n",
        "    as_conv4 = ReLU()(as_conv4)\n",
        "    print('as_4:', as_conv4.shape)\n",
        "\n",
        "    as_conv5 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(4, 1), name='as_conv5')(as_conv4)\n",
        "    as_conv5 = BatchNormalization()(as_conv5)\n",
        "    as_conv5 = ReLU()(as_conv5)\n",
        "    print('as_5:', as_conv5.shape)\n",
        "\n",
        "    as_conv6 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(8, 1), name='as_conv6')(as_conv5)\n",
        "    as_conv6 = BatchNormalization()(as_conv6)\n",
        "    as_conv6 = ReLU()(as_conv6)\n",
        "    print('as_6:', as_conv6.shape)\n",
        "\n",
        "    as_conv7 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(16, 1), name='as_conv7')(as_conv6)\n",
        "    as_conv7 = BatchNormalization()(as_conv7)\n",
        "    as_conv7 = ReLU()(as_conv7)\n",
        "    print('as_7:', as_conv7.shape)\n",
        "\n",
        "    as_conv8 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(32, 1), name='as_conv8')(as_conv7)\n",
        "    as_conv8 = BatchNormalization()(as_conv8)\n",
        "    as_conv8 = ReLU()(as_conv8)\n",
        "    print('as_8:', as_conv8.shape)\n",
        "\n",
        "    as_conv9 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv9')(as_conv8)\n",
        "    as_conv9 = BatchNormalization()(as_conv9)\n",
        "    as_conv9 = ReLU()(as_conv9)\n",
        "    print('as_9:', as_conv9.shape)\n",
        "\n",
        "    as_conv10 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(2, 2), name='as_conv10')(as_conv9)\n",
        "    as_conv10 = BatchNormalization()(as_conv10)\n",
        "    as_conv10 = ReLU()(as_conv10)\n",
        "    print('as_10:', as_conv10.shape)\n",
        "\n",
        "    as_conv11 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(4, 4), name='as_conv11')(as_conv10)\n",
        "    as_conv11 = BatchNormalization()(as_conv11)\n",
        "    as_conv11 = ReLU()(as_conv11)\n",
        "    print('as_11:', as_conv11.shape)\n",
        "\n",
        "    as_conv12 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(8, 8), name='as_conv12')(as_conv11)\n",
        "    as_conv12 = BatchNormalization()(as_conv12)\n",
        "    as_conv12 = ReLU()(as_conv12)\n",
        "    print('as_12:', as_conv12.shape)\n",
        "\n",
        "    as_conv13 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(16, 16), name='as_conv13')(as_conv12)\n",
        "    as_conv13 = BatchNormalization()(as_conv13)\n",
        "    as_conv13 = ReLU()(as_conv13)\n",
        "    print('as_13:', as_conv13.shape)\n",
        "\n",
        "    as_conv14 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(32, 32), name='as_conv14')(as_conv13)\n",
        "    as_conv14 = BatchNormalization()(as_conv14)\n",
        "    as_conv14 = ReLU()(as_conv14)\n",
        "    print('as_14:', as_conv14.shape)\n",
        "\n",
        "    as_conv15 = Convolution2D(8, kernel_size=(1, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv15')(as_conv14)\n",
        "    as_conv15 = BatchNormalization()(as_conv15)\n",
        "    as_conv15 = ReLU()(as_conv15)\n",
        "    print('as_15:', as_conv15.shape)\n",
        "\n",
        "    AS_out = Reshape((298, 8 * 257))(as_conv15)\n",
        "    print('AS_out:', AS_out.shape)\n",
        "    # --------------------------- AS end ---------------------------\n",
        "\n",
        "    # --------------------------- VS_model start ---------------------------\n",
        "    VS_model = Sequential()\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(7, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='vs_conv1'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='vs_conv2'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(2, 1), name='vs_conv3'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(4, 1), name='vs_conv4'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(8, 1), name='vs_conv5'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(16, 1), name='vs_conv6'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Reshape((75, 256, 1)))\n",
        "    VS_model.add(UpSampling2DBilinear((298, 256)))\n",
        "    VS_model.add(Reshape((298, 256)))\n",
        "    # --------------------------- VS_model end ---------------------------\n",
        "\n",
        "    video_input = Input(shape=(75, 1, 1792, people_num))\n",
        "    AVfusion_list = [AS_out]\n",
        "    for i in range(people_num):\n",
        "        single_input = Lambda(sliced, arguments={'index': i})(video_input)\n",
        "        VS_out = VS_model(single_input)\n",
        "        AVfusion_list.append(VS_out)\n",
        "\n",
        "    AVfusion = concatenate(AVfusion_list, axis=2)\n",
        "    AVfusion = TimeDistributed(Flatten())(AVfusion)\n",
        "    print('AVfusion:', AVfusion.shape)\n",
        "\n",
        "    lstm = Bidirectional(LSTM(400, input_shape=(298, 8 * 257), return_sequences=True), merge_mode='sum')(AVfusion)\n",
        "    print('lstm:', lstm.shape)\n",
        "\n",
        "    fc1 = Dense(600, name=\"fc1\", activation='relu', kernel_initializer=he_normal(seed=27))(lstm)\n",
        "    print('fc1:', fc1.shape)\n",
        "    fc2 = Dense(600, name=\"fc2\", activation='relu', kernel_initializer=he_normal(seed=42))(fc1)\n",
        "    print('fc2:', fc2.shape)\n",
        "    fc3 = Dense(600, name=\"fc3\", activation='relu', kernel_initializer=he_normal(seed=65))(fc2)\n",
        "    print('fc3:', fc3.shape)\n",
        "\n",
        "    complex_mask = Dense(257 * 2 * people_num, name=\"complex_mask\", kernel_initializer=glorot_uniform(seed=87))(fc3)\n",
        "    print('complex_mask:', complex_mask.shape)\n",
        "\n",
        "    complex_mask_out = Reshape((298, 257, 2, people_num))(complex_mask)\n",
        "    print('complex_mask_out:', complex_mask_out.shape)\n",
        "\n",
        "    AV_model = Model(inputs=[audio_input, video_input], outputs=complex_mask_out)\n",
        "\n",
        "    # # compile AV_model\n",
        "    # AV_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return AV_model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #############################################################\n",
        "    RESTORE = False\n",
        "    # If set true, continue training from last checkpoint\n",
        "    # needed change 1:h5 file name, 2:epochs num, 3:initial_epoch\n",
        "\n",
        "    # super parameters\n",
        "    people_num = 2\n",
        "    epochs = 50\n",
        "    initial_epoch = 0\n",
        "    batch_size = 1\n",
        "    #############################################################\n",
        "\n",
        "    # audio_input = np.random.rand(5, 298, 257, 2)        # 5 audio parts, (298, 257, 2) stft feature\n",
        "    # audio_label = np.random.rand(5, 298, 257, 2, people_num)     # 5 audio parts, (298, 257, 2) stft feature, people num to be defined\n",
        "\n",
        "    # ///////////////////////////////////////////////////////// #\n",
        "    # create folder to save models\n",
        "    path = './saved_models_AV'\n",
        "    folder = os.path.exists(path)\n",
        "    if not folder:\n",
        "        os.makedirs(path)\n",
        "        print('create folder to save models')\n",
        "    filepath = path + \"/AVmodel-\" + str(people_num) + \"p-{epoch:03d}-{val_loss:.10f}.h5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "\n",
        "    # checkpoint2 = ModelCheckpoint(path + \"/AOmodel-latest-\" + str(people_num) + \".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    # ///////////////////////////////////////////////////////// #\n",
        "\n",
        "    #############################################################\n",
        "    # automatically change lr\n",
        "    def scheduler(epoch):\n",
        "        ini_lr = 0.001\n",
        "        lr = ini_lr\n",
        "        if epoch >= 5:\n",
        "            lr = ini_lr / 5\n",
        "        if epoch >= 10:\n",
        "            lr = ini_lr / 10\n",
        "        return lr\n",
        "\n",
        "\n",
        "    rlr = LearningRateScheduler(scheduler, verbose=1)\n",
        "    #############################################################\n",
        "\n",
        "    # ///////////////////////////////////////////////////////// #\n",
        "    # read train and val file name\n",
        "    # format: mix.npy single.npy single.npy\n",
        "    trainfile = []\n",
        "    valfile = []\n",
        "#    with open('./data/audio/AV_model_database/dataset_train.txt', 'r') as t:\n",
        "    with open('./data/AV_log/AVdataset_train.txt', 'r') as t:\n",
        "        trainfile = t.readlines()\n",
        "\n",
        "    validation = False\n",
        "    if validation:\n",
        "      with open('./data/AV_log/AVdataset_valid.txt', 'r') as v:\n",
        "          valfile = v.readlines()\n",
        "\n",
        "    # ///////////////////////////////////////////////////////// #\n",
        "\n",
        "    # the training steps\n",
        "    def latest_file(dir):\n",
        "        lists = os.listdir(dir)\n",
        "        lists.sort(key=lambda fn: os.path.getmtime(dir + fn))\n",
        "        file_latest = os.path.join(dir, lists[-1])\n",
        "        return file_latest\n",
        "\n",
        "\n",
        "    if RESTORE:\n",
        "        last_file = latest_file('./saved_models_AO/')\n",
        "        AV_model = load_model(last_file)\n",
        "        info = last_file.strip().split('-')\n",
        "        initial_epoch = int(info[-2])\n",
        "        # print(initial_epoch)\n",
        "    else:\n",
        "        AV_model = AV_model(people_num)\n",
        "        adam = optimizers.Adam()\n",
        "        AV_model.compile(optimizer=adam, loss='mse')\n",
        "\n",
        "    train_generator = AVGenerator(trainfile, database_dir_path='./data/', batch_size=batch_size, shuffle=True)\n",
        "    if validation:\n",
        "      val_generator = AVGenerator(valfile, database_dir_path='./', batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    if validation:\n",
        "      AV_model.fit_generator(generator=train_generator,\n",
        "                            validation_data=val_generator,\n",
        "                            epochs=epochs,\n",
        "                            callbacks=[TensorBoard(log_dir='./log_AV'), checkpoint, rlr],\n",
        "                            initial_epoch=initial_epoch\n",
        "                            )\n",
        "    else:\n",
        "      AV_model.fit_generator(generator=train_generator,\n",
        "                            epochs=epochs,\n",
        "                            callbacks=[TensorBoard(log_dir='./log_AV'), checkpoint, rlr],\n",
        "                            initial_epoch=initial_epoch\n",
        "                            )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImJeHf1nSaUJ"
      },
      "source": [
        "## train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaBOzzOKSWAW"
      },
      "source": [
        "# create AV model\n",
        "#############################################################\n",
        "RESTORE = False\n",
        "# If set true, continue training from last checkpoint\n",
        "# needed change 1:h5 file name, 2:epochs num, 3:initial_epoch\n",
        "\n",
        "# super parameters\n",
        "people_num = 2\n",
        "epochs = 100\n",
        "initial_epoch = 0\n",
        "batch_size = 2 # 4 to feed one 16G GPU\n",
        "gamma_loss = 0.1\n",
        "beta_loss = gamma_loss*2\n",
        "\n",
        "# physical devices option to accelerate training process\n",
        "workers = 1 # num of core\n",
        "use_multiprocessing = False\n",
        "NUM_GPU = 1\n",
        "\n",
        "# PATH\n",
        "path = './saved_AV_models' # model path\n",
        "database_dir_path = './data/'\n",
        "#############################################################\n",
        "\n",
        "# create folder to save models\n",
        "folder = os.path.exists(path)\n",
        "if not folder:\n",
        "    os.makedirs(path)\n",
        "    print('create folder to save models')\n",
        "filepath = path + \"/AVmodel-\" + str(people_num) + \"p-{epoch:03d}-{val_loss:.5f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# automatically change lr\n",
        "def scheduler(epoch):\n",
        "    ini_lr = 0.00001\n",
        "    lr = ini_lr\n",
        "    if epoch >= 5:\n",
        "        lr = ini_lr / 5\n",
        "    if epoch >= 10:\n",
        "        lr = ini_lr / 10\n",
        "    return lr\n",
        "\n",
        "rlr = LearningRateScheduler(scheduler, verbose=1)\n",
        "#############################################################\n",
        "# read train and val file name\n",
        "# format: mix.npy single.npy single.npy\n",
        "trainfile = []\n",
        "valfile = []\n",
        "with open((database_dir_path+'AV_log/AVdataset_train.txt'), 'r') as t:\n",
        "    trainfile = t.readlines()\n",
        "with open((database_dir_path+'AV_log/AVdataset_val.txt'), 'r') as v:\n",
        "    valfile = v.readlines()\n",
        "# ///////////////////////////////////////////////////////// #\n",
        "\n",
        "# the training steps\n",
        "if RESTORE:\n",
        "    latest_file = latest_file(path+'/')\n",
        "    AV_model = load_model(latest_file,custom_objects={\"tf\": tf})\n",
        "    info = latest_file.strip().split('-')\n",
        "    initial_epoch = int(info[-2])\n",
        "else:\n",
        "    AV_model = AV.AV_model(people_num)\n",
        "\n",
        "train_generator = AVGenerator(trainfile,database_dir_path= database_dir_path, batch_size=batch_size, shuffle=True)\n",
        "val_generator = AVGenerator(valfile,database_dir_path=database_dir_path, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if NUM_GPU > 1:\n",
        "    parallel_model = ModelMGPU(AV_model,NUM_GPU)\n",
        "    adam = optimizers.Adam()\n",
        "    loss = audio_loss(gamma=gamma_loss,beta=beta_loss,num_speaker=people_num)\n",
        "    parallel_model.compile(loss=loss,optimizer=adam)\n",
        "    print(AV_model.summary())\n",
        "    parallel_model.fit_generator(generator=train_generator,\n",
        "                           validation_data=val_generator,\n",
        "                           epochs=epochs,\n",
        "                           workers = workers,\n",
        "                           use_multiprocessing= use_multiprocessing,\n",
        "                           callbacks=[TensorBoard(log_dir='./log_AV'), checkpoint, rlr],\n",
        "                           initial_epoch=initial_epoch\n",
        "                           )\n",
        "if NUM_GPU <= 1:\n",
        "    adam = optimizers.Adam()\n",
        "    loss = audio_loss(gamma=gamma_loss,beta=beta_loss, num_speaker=people_num)\n",
        "    AV_model.compile(optimizer=adam, loss=loss)\n",
        "    print(AV_model.summary())\n",
        "    AV_model.fit_generator(generator=train_generator,\n",
        "                           validation_data=val_generator,\n",
        "                           epochs=epochs,\n",
        "                           workers = workers,\n",
        "                           use_multiprocessing= use_multiprocessing,\n",
        "                           callbacks=[TensorBoard(log_dir='./log_AV'), checkpoint, rlr],\n",
        "                           initial_epoch=initial_epoch\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcy8Yo-F042y"
      },
      "source": [
        "## evaluate the model and generate the prediction\n",
        "import sys\n",
        "sys.path.append('../lib')\n",
        "from keras.models import load_model\n",
        "from model.lib.model_ops import ModelMGPU\n",
        "import os\n",
        "import scipy.io.wavfile as wavfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# super parameters\n",
        "people_num = 2\n",
        "NUM_GPU = 1\n",
        "\n",
        "# PATH\n",
        "model_path = './saved_models_AO_with_norm/AOmodel-2p-015-0.02258.h5'\n",
        "dir_path = './pred/'\n",
        "if not os.path.isdir(dir_path):\n",
        "    os.mkdir(dir_path)\n",
        "database_path = './data/audio/audio_database/mix/'\n",
        "face_path = './data/video/face_emb/'\n",
        "\n",
        "# load data\n",
        "testfiles = []\n",
        "\n",
        "with open('./data/AV_log/AVdataset_train.txt', 'r') as f:\n",
        "    testfiles = f.readlines()\n",
        "\n",
        "def parse_X_data(line,num_people=people_num,database_path=database_path,face_path=face_path):\n",
        "    parts = line.split() # get each name of file for one testset\n",
        "    mix_str = parts[0]\n",
        "    name_list = mix_str.replace('.npy','')\n",
        "    name_list = name_list.replace('mix-','',1)\n",
        "    names = name_list.split('-')\n",
        "    single_idxs = []\n",
        "    for i in range(num_people):\n",
        "        single_idxs.append(names[i])\n",
        "    file_path = database_path + mix_str\n",
        "    mix = np.load(file_path)\n",
        "    face_embs = np.zeros((1,75,1,1792,num_people))\n",
        "    for i in range(num_people):\n",
        "        face_embs[1,:,:,:,i] = np.load(face_path+\"%05d_face_emb.npy\"%single_idxs[i])\n",
        "\n",
        "    return mix,single_idxs,face_embs\n",
        "\n",
        "\n",
        "# predict data\n",
        "AV_model = load_model(model_path,custom_objects={\"tf\": tf})\n",
        "if NUM_GPU > 1:\n",
        "    parallel_model = ModelMGPU(AV_model,NUM_GPU)\n",
        "    for line in testfiles:\n",
        "        mix,single_idxs,face_embs = parse_X_data(line)\n",
        "        mix_expand = np.expand_dims(mix, axis=0)\n",
        "        cRMs = parallel_model.predict([mix_expand,face_embs])\n",
        "        cRMs = cRMs[0]\n",
        "        prefix = \"\"\n",
        "        for idx in single_idxs:\n",
        "            prefix += idx + \"-\"\n",
        "        for i in range(len(cRMs)):\n",
        "            cRM = cRMs[:,:,:,i]\n",
        "            assert cRM.shape == (298,257,2)\n",
        "            F = utils.fast_icRM(mix,cRM)\n",
        "            T = utils.fast_istft(F,power=False)\n",
        "            filename = dir_path+prefix+str(single_idxs[i])+'.wav'\n",
        "            wavfile.write(filename,16000,T)\n",
        "\n",
        "\n",
        "if NUM_GPU <= 1:\n",
        "    for line in testfiles:\n",
        "        mix, single_idxs, face_embs = parse_X_data(line)\n",
        "        mix_expand = np.expand_dims(mix, axis=0)\n",
        "        cRMs = AV_model.predict([mix_expand, face_embs])\n",
        "        cRMs = cRMs[0]\n",
        "        prefix = \"\"\n",
        "        for idx in single_idxs:\n",
        "            prefix += idx + \"-\"\n",
        "        for i in range(people_num):\n",
        "            cRM = cRMs[:,:,:,i]\n",
        "            assert cRM.shape == (298,257,2)\n",
        "            F = utils.fast_icRM(mix,cRM)\n",
        "            T = utils.fast_istft(F,power=False)\n",
        "            filename = dir_path+prefix+single_idxs[i]+'.wav'\n",
        "            wavfile.write(filename,16000,T)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19UDKZh-HU4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}